{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ZGDhAGPHQVd"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "import os\n",
        "import cv2\n",
        "from keras.layers import Dense, Flatten\n",
        "\n",
        "# Define the paths to your image folders\n",
        "train_path = 'C:\\\\Users\\\\Anil\\\\Downloads\\\\train-20230429T163424Z-001\\\\'\n",
        "val_path = 'C:\\\\Users\\\\Anil\\\\Downloads\\\\val-20230429T162433Z-001\\\\'\n",
        "\n",
        "# Set the path to the folder containing the 'train' folder\n",
        "data_dir = train_path\n",
        "# Set the image size\n",
        "img_size = (32, 32)\n",
        "# Create empty lists for the images and labels\n",
        "images = []\n",
        "labels = []\n",
        "# Loop over each folder from '0' to '9'\n",
        "for label in range(10):\n",
        " folder_path = os.path.join(data_dir, 'train', str(label))\n",
        " # Loop over each image in the folder\n",
        " for file in os.listdir(folder_path):\n",
        "   file_path = os.path.join(folder_path, file)\n",
        "   if file_path.endswith(('.tiff','.bmp')):\n",
        "     img = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE)\n",
        "     img = cv2.resize(img, img_size)\n",
        " # Append the image and label to the lists\n",
        " images.append(img)\n",
        " labels.append(label)\n",
        "# Convert the lists to NumPy arrays\n",
        "\n",
        "images = np.array(images)\n",
        "labels = np.array(labels)\n",
        "# Save the arrays in NumPy format\n",
        "np.save('x_train.npy', images)\n",
        "np.save('y_train.npy', labels)\n",
        "\n",
        "# Set the path to the folder containing the 'val' folder\n",
        "data_dir_val = val_path\n",
        "# Set the image size\n",
        "img_size_val = (32, 32)\n",
        "# Create empty lists for the images and labels\n",
        "images_val = []\n",
        "labels_val = []\n",
        "# Loop over each folder from '0' to '9'\n",
        "for label in range(10):\n",
        " folder_path = os.path.join(data_dir_val, 'val\\\\', str(label))\n",
        " for file in os.listdir(folder_path):\n",
        "   file_path = os.path.join(folder_path, file)\n",
        "   if file_path.endswith(('.tiff','.bmp')):\n",
        "     img = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE)\n",
        "     img = cv2.resize(img, img_size_val)\n",
        " \n",
        " images_val.append(img)\n",
        " labels_val.append(label)\n",
        "# Convert the lists to NumPy arrays\n",
        "images_val = np.array(images_val)\n",
        "labels_val = np.array(labels_val)\n",
        "# Save the arrays in NumPy format\n",
        "np.save('x_test.npy', images_val)\n",
        "np.save('y_test.npy', labels_val)\n",
        "\n",
        "# Load the dataset\n",
        "x_train = np.load('x_train.npy')\n",
        "y_train = np.load('y_train.npy')\n",
        "x_test = np.load('x_test.npy')\n",
        "y_test = np.load('y_test.npy')\n",
        "\n",
        "# test the images are loaded correctly\n",
        "print(len(x_train))\n",
        "print(len(x_test))\n",
        "x_train[0].shape\n",
        "x_train[0]\n",
        "plt.matshow(x_train[0])\n",
        "plt.matshow(x_train[999])\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)\n",
        "y_train\n",
        "y_test\n",
        "plt.matshow(x_test[150])\n",
        "\n",
        "\n",
        "# # flatten the dataset i.e, change 2D to 1D (skipped this , and flattened in the model)\n",
        "# x_train_flat = x_train.reshape(len(x_train),32*32)\n",
        "# x_test_flat = x_test.reshape(len(x_test),32*32)\n",
        "# print(x_train_flat.shape)\n",
        "# print(x_test_flat.shape)\n",
        "# x_train_flat[0]\n",
        "\n",
        "# creating a simple nn\n",
        "# create a dense layer where every input is connected to every other output, the number of inputs are 1000, outputs are 10\n",
        "# activation function is sigmoid\n",
        "model = keras.Sequential([\n",
        " keras.layers.Flatten(),\n",
        " keras.layers.Dense(10, input_shape=(1024,),activation = 'sigmoid')\n",
        "])\n",
        "# compile the nn\n",
        "model.compile(optimizer='adam',\n",
        " loss='sparse_categorical_crossentropy',\n",
        " metrics=['accuracy']\n",
        " )\n",
        "# train the model\n",
        "# some 10 iterations done here\n",
        "model.fit(x_train, y_train,epochs= 10, validation_data=(x_test, y_test))\n",
        "# Observation : we see a better accuracy from the 2nd iteration\n",
        "\n",
        "# now scale and try to check the accuracy, divide dataset by 255\n",
        "x_train_scaled = x_train/255\n",
        "x_test_scaled = x_test/255\n",
        "model.fit(x_train_scaled, y_train,epochs= 10, validation_data=(x_test_scaled, y_test))\n",
        "\n",
        "# Observation : we got better result for all iterations on scaling the training dataset\n",
        "\n",
        "# evaluate test dataset\n",
        "model.evaluate(x_test_scaled,y_test)\n",
        "\n",
        "# Observation : result almost same as the training dataset,\n",
        "\n",
        "# predict 1st image\n",
        "plt.matshow(x_test[0])\n",
        "y_predicted = model.predict(x_test_scaled)\n",
        "y_predicted[0]\n",
        "# this showing the 10 results for the input '0', we need to look for the value which is max\n",
        "print('Predicted Value is ',np.argmax(y_predicted[0]))\n",
        "# test some more values\n",
        "plt.matshow(x_test[88])\n",
        "print('Predicted Value is ',np.argmax(y_predicted[88]))\n",
        "plt.matshow(x_test[177])\n",
        "print('Predicted Value is ',np.argmax(y_predicted[177]))\n",
        "\n",
        "# some predictions may not be not right\n",
        "# build confusion matrix to see how our prediction looks like\n",
        "# convert to concrete values\n",
        "y_predicted_labels=[np.argmax(i) for i in y_predicted]\n",
        "print(y_predicted_labels, len(y_predicted_labels))\n",
        "conf_mat = tf.math.confusion_matrix(labels=y_test, predictions=y_predicted_labels)\n",
        "conf_mat\n",
        "\n",
        "\n",
        "import seaborn as sn\n",
        "plt.figure(figsize = (10,10))\n",
        "sn.heatmap(conf_mat,annot=True,fmt='d')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "\n",
        "# here we can see there are some errors\n",
        "# we need to modify our nn, we add some layers in the above model and different activation function\n",
        "\n",
        "# in 1st Dense layer,the input is 32 x 32 = 1024 neurons, which will give 10 output(numbers from 0 to 9)\n",
        "# 2nd Dense layer,the input is 10 neurons from above layers output\n",
        "# we can add more layers for accuracy\n",
        "model2 = keras.Sequential([\n",
        " keras.layers.Flatten(),\n",
        " keras.layers.Dense(1024,input_shape=(1024,), activation='relu'),\n",
        " keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "# compile the nn\n",
        "model2.compile(optimizer='adam',\n",
        " loss='sparse_categorical_crossentropy',\n",
        " metrics=['accuracy']\n",
        " )\n",
        "# train the model\n",
        "# some 10 iterations done here\n",
        "history = model2.fit(x_train_scaled, y_train,epochs= 10, validation_data=(x_test_scaled, y_test))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Observation : due to multiple layers the compiling will take more time to execute\n",
        "# we also got amazing accuracy than earlier\n",
        "# evaluate test dataset on modified model\n",
        "model2.evaluate(x_test_scaled,y_test)\n",
        "# redo the confusion matrix\n",
        "# build confusion matrix to see how our prediction looks like\n",
        "# convert to concrete values\n",
        "y_predicted = model2.predict(x_test_scaled)\n",
        "y_predicted[0]\n",
        "y_predicted_labels=[np.argmax(i) for i in y_predicted]\n",
        "print(y_predicted_labels, len(y_predicted_labels))\n",
        "conf_mat = tf.math.confusion_matrix(labels=y_test, predictions=y_predicted_labels)\n",
        "conf_mat\n",
        "plt.figure(figsize = (10,10))\n",
        "sn.heatmap(conf_mat,annot=True,fmt='d')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "\n",
        "# Observatoin : we see in the updated model, there are less number of errors,\n",
        "# whatever is not in diagonal is a error\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print('Test accuracy:', test_acc)\n",
        "# Plot the training and validation accuracy\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()\n"
      ]
    }
  ]
}